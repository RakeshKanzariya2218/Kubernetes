apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nodeport-deployment           # name of deployment 
spec:                                          
  replicas: 3                            # 3 pods will create 
  selector:
    matchLabels:
      app: myapp                         # label should match with pod label 
  template:
    metadata:
      # name : < pod name >  but here pod name is not allowed because deployment ymal is create pood with autoname 
      labels:
        app: myapp                      # label should match with deployment label 
        type: frontend
    spec:
      containers:
        - name: nginx-container
          image: nginx                            ## if you want to use custom image then ; image : repository url : < image tag/name >  or , image : repository url @ digest 
                                                  ## ex :-   image : 451947743144.dkr.ecr.us-east-1.amazonaws.com/my-image:latest    or      , image : 451947743144.dkr.ecr.us-east-1.amazonaws.com/my-image@sha256:b22f8a31d499f51d48ae488184837ad2c2525f6d9dafa99c6ef05cc09ba02b29
                                                  ## image : url@digest    = this is safe way in this if image change  then no problem 
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort                 # nodeport type serrvice which used for  external communication like user hit the nodeip:nodeport 
  selector:
    app: myapp                    # label should match with pod label so service control this  label pod and resirecting request to that pod 
  ports:
    - targetPort: 80               # port of pod (nginx )
      port: 80                     # serrvise port ( service port and pod port both are same is recommonded way to configure for better understanding )
      nodePort: 30008              # node port which is allocated to all node inside the cluster ( a comman port is allocated to all nodes so using samme sg ) 



      #######  service is work at cluster level so  any node have not running pod and end user hit that node ip but inside that node no any pod are running so request go to iptable ( kubeproxy ) by
      #        service concept , so any node ip will hit service work as load balancer but in real time node are private and not give any external access as node ip 
 
